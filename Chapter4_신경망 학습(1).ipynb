{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미니배치 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 4.2.3 미니배치 학습\n",
    "# 훈련 데이터 전체에 대한 오차함수\n",
    "# E = -1/N * ∑ _n (∑ _k (tk * log(yk)))\n",
    "# N : 데이터의 개수\n",
    "# 훈련 데이터 전체에 대한 손실 함수를 계산하기에는 시간이 오래걸리기 때문에\n",
    "# 일부를 추려 전체의 근사치로 이용할 수 있다.\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=False)\n",
    "\n",
    "print(x_train.shape)  # (60000, 784)\n",
    "print(t_train.shape)  # 원-핫 인코딩 된 정답 레이블 (60000, 10)\n",
    "\n",
    "# 무작위 10개 추출\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "\n",
    "# 4.2.4 (배치용) 교차 엔트로피 오차 구현하기\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "\n",
    "# 4.2.5 왜 손실 함수를 설정하는가?\n",
    "# 신경망을 학습할 때 정확도를 지표로 삼아서는 안 된다.\n",
    "# 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다.\n",
    "# (매개변수의 미소한 변화에는 거의 반응을 보이지 않고 그 값이 분연속적으로 변화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "'''\n",
    "손실 함수loss function : 신경망 성능의 '나쁨'을 나타내는 지표.\n",
    "일반적으로 평균 제곱 오차와 교차 엔트로피 오차를 사용\n",
    "'''\n",
    "\n",
    "\n",
    "# 4.2.1 평균 제곱 오차\n",
    "# E = 1/2 * ∑ _k (yk-tk)²\n",
    "# yk : 신경망의 출력\n",
    "# tk : 정답 레이블\n",
    "# k : 데이터의 차원 수\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "\n",
    "# 정답은 '2'\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# ex1 '2'일 확률이 가장 높다고 추정함(0.6)\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "mse = mean_squared_error(np.array(y), np.array(t))\n",
    "print(mse)  # 0.0975\n",
    "# ex2 '7'일 확률이 가장 높다고 추정함(0.6)\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "mse = mean_squared_error(np.array(y), np.array(t))\n",
    "print(mse)  # 0.5975\n",
    "\n",
    "\n",
    "# 4.2.2 교차 엔트로피 오차\n",
    "# E = -∑ _k (tk * log(yk))\n",
    "# log : 자연로그\n",
    "# yk : 신경망의 출력\n",
    "# tk : 정답 레이블(one-hot encoding)\n",
    "# k : 데이터의 차원 수\n",
    "# 실질적으로 정답일때의 추정의 자연로그를 계산하는 식이 됨\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7  # 0일때 -무한대가 되지 않기 위해 작은 값을 더함\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "\n",
    "# 동일한 계산\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cee = cross_entropy_error(np.array(y), np.array(t))\n",
    "print(cee)  # 0.510825457099\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cee = cross_entropy_error(np.array(y), np.array(t))\n",
    "print(cee)  # 2.30258409299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수치 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "# 4.3.1 미분\n",
    "# 나쁜 구현 예\n",
    "def numerical_diff_bad(f, x):\n",
    "    h = 10e-50\n",
    "    return (f(x + h) - f(x)) / h\n",
    "# h값이 너무 작아 반올림 오차를 일으킬 수 있음 10e-4정도가 적당하다고 알려짐\n",
    "# 전방 차분에서는 차분이 0이 될 수 없어 오차가 발생\n",
    "#  -> 오차를 줄이기 위해 중심 차분을 사용\n",
    "\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 10e-4\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "\n",
    "# 4.3.2 수치 미분의 예\n",
    "# y = 0.01x² + 0.1x\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)  # 0에서 20까지 간격 0.1인 배열 x를 만든다.\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "# plt.show()\n",
    "\n",
    "# x = 5, 10일때 미분\n",
    "print(numerical_diff(function_1, 5))   # 0.200000000000089\n",
    "print(numerical_diff(function_1, 10))  # 0.29999999999996696\n",
    "\n",
    "\n",
    "# 접선의 함수를 구하는 함수\n",
    "def tangent_line(f, x):\n",
    "        d = numerical_diff(f, x)\n",
    "        # print(d)\n",
    "        y = f(x) - d*x\n",
    "        return lambda t: d*t + y\n",
    "\n",
    "\n",
    "tf = tangent_line(function_1, 5)\n",
    "y2 = tf(x)\n",
    "plt.plot(x, y2)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 4.3.3 편미분\n",
    "# f(x0, x1) = x0² + x1²\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "    # or return np.sum(x**2)\n",
    "\n",
    "\n",
    "# x0 = 3, x1 = 4일 때, x0에 대한 편미분을 구하라.\n",
    "def function_tmp1(x0):\n",
    "    return x0**2 + 4.0**2.0\n",
    "\n",
    "\n",
    "# x0 = 3, x1 = 4일 때, x1에 대한 편미분을 구하라.\n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1 * x1\n",
    "\n",
    "\n",
    "print(numerical_diff(function_tmp1, 3.0))  # 5.999999999998451\n",
    "print(numerical_diff(function_tmp2, 4.0))  # 8.000000000000895"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

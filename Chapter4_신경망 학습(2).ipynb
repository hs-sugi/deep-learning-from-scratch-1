{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "# 앞 절에서 x0, x1에 대한 편미분을 변수별로 따로 계산했음.\n",
    "# x0, x1의 편미분을 동시에 계산하고 싶다면?\n",
    "# x0 = 3, x1 = 4일 때 (x0, x1) 양쪽의 편미분을 묶어 벡터로 정리한 것을 기울기gradient라고 한다.\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)  # x와 형상이 같은 배열을 생성\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val  # 값 복원\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "# f(x0, x1) = x0² + x1²\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "    # or return np.sum(x**2)\n",
    "\n",
    "\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))  # [ 6.  8.]\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))  # [ 0.  4.]\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))  # [ 6.  0.]\n",
    "\n",
    "# 4.4.1 경사법(경사 하강법)\n",
    "# x0 = x0 - η*∂f/∂x0\n",
    "# x1 = x1 - η*∂f/∂x1\n",
    "# η(eta) : 갱신하는 양, 학습률learning rate\n",
    "# 위 식을 반복\n",
    "\n",
    "\n",
    "# f:최적화하려는 함수\n",
    "# init_x : 초깃값\n",
    "# lr : 학습률\n",
    "# step_num : 반복횟수\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "\n",
    "# 경사법으로 f(x0, x1) = x0² + x1²의 최솟값을 구해라\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=0.1)\n",
    "print(x)  # [ -6.11110793e-10   8.14814391e-10]\n",
    "\n",
    "# 학습률이 너무 큼\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=10.0)\n",
    "print(x)  # [ -2.58983747e+13  -1.29524862e+12] 발산함\n",
    "\n",
    "# 학습률이 너무 작음\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=1e-10)\n",
    "print(x)  # [-2.99999994  3.99999992] 거의 변화 없음\n",
    "\n",
    "# 그래프\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=0.1, step_num=20)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], '--b')\n",
    "plt.plot([0, 0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:, 0], x_history[:, 1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.\n",
    "- 신경망 학습은 다음과 같이 4단계로 수행한다.\n",
    "\n",
    "#### 1단계 - 미니배치\n",
    "- 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실함수 값을 줄이는 것이 목표이다.\n",
    "#### 2단계 - 기울기 산출\n",
    "- 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.\n",
    "- 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "#### 3단계 - 매개변수 갱신\n",
    "- 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "#### 4단계 - 반복\n",
    "- 1~3단계를 반복한다.\n",
    "- 데이터를 무작위로 선정하기 때문에 확률적 경사 하강법stochastic gradient descent, SGD라고 부른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import sigmoid, softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \"\"\"\n",
    "    params : 신경망의 매개변수를 보관하는 딕셔너리 변수.\n",
    "    params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향.\n",
    "    params['W2']은 2번째 층의 가중치, params['b2']은 2번째 층의 편향.\n",
    "    grad : 기울기를 보관하는 딕셔너리 변수(numerical_gradient()의 반환값)\n",
    "    grads['W1']은 1번째 층의 가중치의 기울기, grads['b1']은 1번째 층의 편향의 기울기.\n",
    "    grads['W2']은 2번째 층의 가중치의 기울기, grads['b2']은 2번째 층의 편향의 기울기.\n",
    "    \"\"\"\n",
    "    # 초기화를 수행한다.\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                 weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    # 예측(추론)을 수행한다.\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # 손실 함수의 값을 구한다.\n",
    "    # x : 입력데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    # 정확도를 구한다.\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # 가중치 매개변수의 기울기를 구한다.\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "    print(net.params['W1'].shape)  # (784, 100)\n",
    "    print(net.params['b1'].shape)  # (100,)\n",
    "    print(net.params['W2'].shape)  # (100, 10)\n",
    "    print(net.params['b2'].shape)  # (10,)\n",
    "\n",
    "    x = np.random.rand(100, 784)  # 더미 입력 데이터(100장 분량)\n",
    "    t = np.random.rand(100, 10)   # 더미 정답 레이블(100장 분량)\n",
    "\n",
    "    grads = net.numerical_gradient(x, t)  # 기울기 계산\n",
    "    # 주의 : 실행하는데 아주 오래걸림\n",
    "    print(grads['W1'].shape)  # (784, 100)\n",
    "    print(grads['b1'].shape)  # (100,)\n",
    "    print(grads['W2'].shape)  # (100, 10)\n",
    "    print(grads['b2'].shape)  # (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망에서의 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "# 4.4.2 신경망에서의 기울기\n",
    "class simpleNet:\n",
    "    \"\"\"docstring for simpleNet\"\"\"\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)  # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "net = simpleNet()\n",
    "print(net.W)  # 가중치 매개변수(랜덤)\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "print(np.argmax(p))  # 최댓값의 인덱스\n",
    "\n",
    "t = np.array([0, 0, 1])  # 정답 레이블\n",
    "print(net.loss(x, t))\n",
    "\n",
    "\n",
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "# 4.4.2 신경망에서의 기울기\n",
    "class simpleNet:\n",
    "    \"\"\"docstring for simpleNet\"\"\"\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)  # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "net = simpleNet()\n",
    "print(net.W)  # 가중치 매개변수(랜덤)\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "print(np.argmax(p))  # 최댓값의 인덱스\n",
    "\n",
    "t = np.array([0, 0, 1])  # 정답 레이블\n",
    "print(net.loss(x, t))\n",
    "\n",
    "\n",
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미니배치 학습 구현 / 시험 데이터 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 60000개의 훈련 데이터에서 임의로 100개의 데이터(이미지&정답 레이블)을 추려냄.\n",
    "- 100개의 미니배치를 대상으로 확률적 경사 하강법을 수행해 매개변수를 갱신한다.\n",
    "- 경사법에 의한 갱신 횟수를 1000번으로 설정하고 갱신할 때마다 손실 함수를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "\n",
    "# 4.5.2 미니배치 학습 구현하기\n",
    "# * 주의 : 아주 오래 걸림 *\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=False)\n",
    "\n",
    "# 하이퍼 파라메터\n",
    "iters_num = 1000  # 반복횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100  # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    print(i)\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch)  # 다음 장에서 구현할 더 빠른 방법!\n",
    "\n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 1에폭 당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \"\n",
    "              + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# print(train_loss_list)\n",
    "\n",
    "\n",
    "# 4.5.3 시험 데이터로 평가하기\n",
    "\"\"\"\n",
    "위의 계산에서 손실 함수의 값이 점점 감소하게 되는데, 이때의 손실 함수는\n",
    "훈련 데이터의 미니배치에 대한 손실 함수를 말한다.\n",
    "훈련 데이터 외의 데이터를 올바르게 인식하는지(오버피팅이 일어나지 않았는지) 확인 필요.\n",
    "1 에폭별로 훈련 데이터와 시험 데이터에 대한 정확도를 기록하도록 수정.\n",
    "에폭epoch : 학습에서 훈련 데이터를 모두 소진했을 때의 횟수.\n",
    "10000개를 100개의 미니배치로 학슬할 경우 100회가 1에폭이 된다.\n",
    "훈련 데이터와 시험 데이터의 정확도 추이가 비슷하다면 오버피팅이 일어나지 않은 것이다.\n",
    "오버피팅이 발생했다면, 어느 순간부터 시험 데이터에 대한 정확도가 떨어지기 시작한다.\n",
    "오버피팅이 발생하기 전에 학습을 중단해 오버피팅을 예방하는 기법을 조기 종료early stopping라고 한다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 계산에서 손실 함수의 값이 점점 감소하게 되는데, 이때의 손실 함수는 훈련 데이터의 미니배치에 대한 손실 함수를 말한다.\n",
    "- 훈련 데이터 외의 데이터를 올바르게 인식하는지(오버피팅이 일어나지 않았는지) 확인이 필요하다.\n",
    "- 1 에폭별로 훈련 데이터와 시험 데이터에 대한 정확도를 기록하도록 수정.\n",
    "  - 에폭epoch : 학습에서 훈련 데이터를 모두 소진했을 때의 횟수.\n",
    "  - 10000개를 100개의 미니배치로 학슬할 경우 100회가 1에폭이 된다.\n",
    "- 훈련 데이터와 시험 데이터의 정확도 추이가 비슷하다면 오버피팅이 일어나지 않은 것이다.\n",
    "- 오버피팅이 발생했다면, 어느 순간부터 시험 데이터에 대한 정확도가 떨어지기 시작한다.\n",
    "- 오버피팅이 발생하기 전에 학습을 중단해 오버피팅을 예방하는 기법을 조기 종료early stopping라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2층 신경망 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "\n",
    "        batch_num = x.shape[0]\n",
    "\n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        # backward\n",
    "        dy = (y - t) / batch_num  # ????\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "\n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
